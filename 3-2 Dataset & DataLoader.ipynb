{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch通常使用Dataset和DataLoader这两个工具类来构建数据管道。\n",
    "\n",
    "Dataset定义了数据集的内容，它相当于一个类似列表的数据结构，具有确定的长度，能够用索引获取数据集中的元素。\n",
    "\n",
    "而DataLoader定义了按batch加载数据集的方法，它是一个实现了`__iter__`方法的可迭代对象，每次迭代输出一个batch的数据。\n",
    "\n",
    "DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。\n",
    "\n",
    "在绝大部分情况下，用户只需实现Dataset的`__len__`方法和`__getitem__`方法，就可以轻松构建自己的数据集，并用默认数据管道进行加载。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、Dataset和DataLoader概述\n",
    "\n",
    "### 1. 获取一个batch的步骤\n",
    "\n",
    "(假定数据集的特征和标签分别表示为张量`X`和`Y`，数据集可以表示为`(X,Y)`, 假定batch大小为`m`)\n",
    "\n",
    "1，首先我们要确定数据集的长度`n`。\n",
    "\n",
    "结果类似：`n = 1000`。\n",
    "\n",
    "2，然后我们从`0`到`n-1`的范围中抽样出`m`个数(batch大小)。\n",
    "\n",
    "假定`m=4`, 拿到的结果是一个列表，类似：`indices = [1,4,8,9]`\n",
    "\n",
    "3，接着我们从数据集中去取这`m`个数对应下标的元素。\n",
    "\n",
    "拿到的结果是一个元组列表，类似：`samples = [(X[1],Y[1]),(X[4],Y[4]),(X[8],Y[8]),(X[9],Y[9])]`\n",
    "\n",
    "4，最后我们将结果整理成两个张量作为输出。\n",
    "\n",
    "拿到的结果是两个张量，类似`batch = (features,labels) `， \n",
    "\n",
    "其中 `features = torch.stack([X[1],X[4],X[8],X[9]])`\n",
    "\n",
    "`labels = torch.stack([Y[1],Y[4],Y[8],Y[9]])`\n",
    "\n",
    "### 2. Dataset和DataLoader的功能分工\n",
    "\n",
    "上述第1个步骤确定数据集的长度是由 Dataset的`__len__` 方法实现的。\n",
    "\n",
    "第2个步骤从`0`到`n-1`的范围中抽样出`m`个数的方法是由 DataLoader的 `sampler`和 `batch_sampler`参数指定的。\n",
    "\n",
    "`sampler`参数指定单个元素抽样方法，一般无需用户设置，程序默认在DataLoader的参数`shuffle=True`时采用随机抽样，`shuffle=False`时采用顺序抽样。\n",
    "\n",
    "`batch_sampler`参数将多个抽样的元素整理成一个列表，一般无需用户设置，默认方法在DataLoader的参数`drop_last=True`时会丢弃数据集最后一个长度不能被batch大小整除的批次，在`drop_last=False`时保留最后一个批次。\n",
    "\n",
    "第3个步骤的核心逻辑根据下标取数据集中的元素 是由 Dataset的 `__getitem__`方法实现的。\n",
    "\n",
    "第4个步骤的逻辑由DataLoader的参数`collate_fn`指定。一般情况下也无需用户设置。\n",
    "\n",
    "### 3. Dataset和DataLoader的主要接口\n",
    "\n",
    "以下是 Dataset和 DataLoader的核心接口逻辑伪代码，不完全和源码一致。\n",
    "\n",
    "```python\n",
    "import torch \n",
    "class Dataset(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self,dataset,batch_size,collate_fn,shuffle = True,drop_last = False):\n",
    "        self.dataset = dataset\n",
    "        self.sampler =torch.utils.data.RandomSampler if shuffle else \\\n",
    "           torch.utils.data.SequentialSampler\n",
    "        self.batch_sampler = torch.utils.data.BatchSampler\n",
    "        self.sample_iter = self.batch_sampler(\n",
    "            self.sampler(range(len(dataset))),\n",
    "            batch_size = batch_size,drop_last = drop_last)\n",
    "        \n",
    "    def __next__(self):\n",
    "        indices = next(self.sample_iter)\n",
    "        batch = self.collate_fn([self.dataset[i] for i in indices])\n",
    "        return batch\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 二、使用Dataset创建数据集\n",
    "\n",
    "<!-- #region -->\n",
    "Dataset创建数据集常用的方法有：\n",
    "\n",
    "* 使用 torch.utils.data.TensorDataset 根据Tensor创建数据集(numpy的array，Pandas的DataFrame需要先转换成Tensor)。\n",
    "\n",
    "* 使用 torchvision.datasets.ImageFolder 根据图片目录创建图片数据集。\n",
    "\n",
    "* 继承 torch.utils.data.Dataset 创建自定义数据集。\n",
    "\n",
    "\n",
    "此外，还可以通过\n",
    "\n",
    "* torch.utils.data.random_split 将一个数据集分割成多份，常用于分割训练集，验证集和测试集。\n",
    "\n",
    "* 调用Dataset的加法运算符(`+`)将多个数据集合并成一个数据集。\n",
    "<!-- #endregion -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 根据Tensor创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.TensorDataset'>\n",
      "<class 'torch.utils.data.dataset.Subset'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split \n",
    "\n",
    "# 根据Tensor创建数据集\n",
    "\n",
    "from sklearn import datasets \n",
    "iris = datasets.load_iris()\n",
    "ds_iris = TensorDataset(torch.tensor(iris.data),torch.tensor(iris.target))\n",
    "\n",
    "# 分割成训练集和预测集\n",
    "n_train = int(len(ds_iris)*0.8)\n",
    "n_valid = len(ds_iris) - n_train\n",
    "ds_train, ds_valid = random_split(ds_iris, [n_train, n_valid])\n",
    "\n",
    "print(type(ds_iris))\n",
    "print(type(ds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.7000, 3.1000, 5.6000, 2.4000],\n",
      "        [6.2000, 2.2000, 4.5000, 1.5000],\n",
      "        [7.7000, 3.8000, 6.7000, 2.2000],\n",
      "        [6.3000, 2.9000, 5.6000, 1.8000],\n",
      "        [6.4000, 3.2000, 4.5000, 1.5000],\n",
      "        [5.5000, 2.5000, 4.0000, 1.3000],\n",
      "        [6.9000, 3.1000, 4.9000, 1.5000],\n",
      "        [6.0000, 3.4000, 4.5000, 1.6000]], dtype=torch.float64) tensor([2, 1, 2, 2, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# 使用DataLoader加载数据集\n",
    "\n",
    "dl_train, dl_valid = DataLoader(ds_train, batch_size = 8), DataLoader(ds_valid, batch_size = 8)\n",
    "\n",
    "for features,labels in dl_train:\n",
    "    print(features,labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(ds_train) =  120\n",
      "len(ds_valid) =  30\n",
      "len(ds_train+ds_valid) =  150\n",
      "<class 'torch.utils.data.dataset.ConcatDataset'>\n"
     ]
    }
   ],
   "source": [
    "# 演示加法运算符（`+`）的合并作用\n",
    "\n",
    "ds_data = ds_train + ds_valid\n",
    "\n",
    "print('len(ds_train) = ', len(ds_train))\n",
    "print('len(ds_valid) = ', len(ds_valid))\n",
    "print('len(ds_train+ds_valid) = ', len(ds_data))\n",
    "\n",
    "print(type(ds_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 根据图片目录创建图片数据集\n",
    "\n",
    "```python\n",
    "import numpy as np \n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms,datasets \n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "#演示一些常用的图片增强操作\n",
    "```\n",
    "\n",
    "```python\n",
    "from PIL import Image\n",
    "img = Image.open('./data/cat.jpeg')\n",
    "img\n",
    "```\n",
    "\n",
    "![](./data/5-1-傻乎乎.png)\n",
    "\n",
    "```python\n",
    "# 随机数值翻转\n",
    "transforms.RandomVerticalFlip()(img)\n",
    "```\n",
    "\n",
    "![](./data/5-1-翻转.png)\n",
    "\n",
    "```python\n",
    "#随机旋转\n",
    "transforms.RandomRotation(45)(img)\n",
    "```\n",
    "\n",
    "![](./data/5-1-旋转.png)\n",
    "\n",
    "```python\n",
    "# 定义图片增强操作\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "   transforms.RandomHorizontalFlip(), #随机水平翻转\n",
    "   transforms.RandomVerticalFlip(), #随机垂直翻转\n",
    "   transforms.RandomRotation(45),  #随机在45度角度内旋转\n",
    "   transforms.ToTensor() #转换成张量\n",
    "  ]\n",
    ") \n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "  ]\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "# 根据图片目录创建数据集\n",
    "ds_train = datasets.ImageFolder(\"./data/cifar2/train/\",\n",
    "            transform = transform_train,target_transform= lambda t:torch.tensor([t]).float())\n",
    "ds_valid = datasets.ImageFolder(\"./data/cifar2/test/\",\n",
    "            transform = transform_train,target_transform= lambda t:torch.tensor([t]).float())\n",
    "\n",
    "print(ds_train.class_to_idx)\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "{'0_airplane': 0, '1_automobile': 1}\n",
    "```\n",
    "\n",
    "```python\n",
    "# 使用DataLoader加载数据集\n",
    "\n",
    "dl_train = DataLoader(ds_train,batch_size = 50,shuffle = True,num_workers=3)\n",
    "dl_valid = DataLoader(ds_valid,batch_size = 50,shuffle = True,num_workers=3)\n",
    "```\n",
    "\n",
    "```python\n",
    "for features,labels in dl_train:\n",
    "    print(features.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "```\n",
    "\n",
    "```\n",
    "torch.Size([50, 3, 32, 32])\n",
    "torch.Size([50, 1])\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 创建自定义数据集\n",
    "\n",
    "通过继承Dataset类创建imdb文本分类任务的自定义数据集。\n",
    "\n",
    "思路如下：首先，对训练集文本分词构建词典。然后将训练集文本和测试集文本数据转换成token单词编码。\n",
    "\n",
    "接着将转换成单词编码的训练集数据和测试集数据按样本分割成多个文件，一个文件代表一个样本。\n",
    "\n",
    "最后，我们可以根据文件名列表获取对应序号的样本内容，从而构建Dataset数据集。\n",
    "\n",
    "见`jupyter notebook 1-3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、使用DataLoader加载数据集\n",
    "\n",
    "\n",
    "DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。\n",
    "\n",
    "DataLoader的函数签名如下。\n",
    "\n",
    "<!-- #region -->\n",
    "```python\n",
    "DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    batch_sampler=None,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    timeout=0,\n",
    "    worker_init_fn=None,\n",
    "    multiprocessing_context=None,\n",
    ")\n",
    "```\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "一般情况下，我们仅仅会配置 dataset, batch_size, shuffle, num_workers, drop_last这五个参数，其他参数使用默认值即可。\n",
    "\n",
    "DataLoader除了可以加载我们前面讲的 torch.utils.data.Dataset 外，还能够加载另外一种数据集 torch.utils.data.IterableDataset。\n",
    "\n",
    "和Dataset数据集相当于一种列表结构不同，IterableDataset相当于一种迭代器结构。 它更加复杂，一般较少使用。\n",
    "\n",
    "- dataset : 数据集\n",
    "- batch_size: 批次大小\n",
    "- shuffle: 是否乱序\n",
    "- sampler: 样本采样函数，一般无需设置。\n",
    "- batch_sampler: 批次采样函数，一般无需设置。\n",
    "- num_workers: 使用多进程读取数据，设置的进程数。\n",
    "- collate_fn: 整理一个批次数据的函数。\n",
    "- pin_memory: 是否设置为锁业内存。默认为False，锁业内存不会使用虚拟内存(硬盘)，从锁业内存拷贝到GPU上速度会更快。\n",
    "- drop_last: 是否丢弃最后一个样本数量不足batch_size批次数据。\n",
    "- timeout: 加载一个数据批次的最长等待时间，一般无需设置。\n",
    "- worker_init_fn: 每个worker中dataset的初始化函数，常用于 IterableDataset。一般不使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 14, 15, 37,  9, 24, 45, 28, 48, 23])\n",
      "tensor([21, 40,  6,  8, 19, 49,  2, 46, 42, 20])\n",
      "tensor([39, 29, 38, 32, 12,  3, 13, 17, 41, 25])\n",
      "tensor([16,  4, 18,  5, 44, 27, 10, 36, 43, 30])\n"
     ]
    }
   ],
   "source": [
    "#构建输入数据管道\n",
    "ds = TensorDataset(torch.arange(1,50))\n",
    "dl = DataLoader(ds,\n",
    "                batch_size = 10,\n",
    "                shuffle= True,\n",
    "                num_workers=2,\n",
    "                drop_last = True)\n",
    "\n",
    "#迭代数据\n",
    "for batch, in dl:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
