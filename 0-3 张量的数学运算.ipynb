{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0-3 张量的数学运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的数学运算符可以分为标量运算符、向量运算符、以及矩阵运算符。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、标量运算\n",
    "加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。\n",
    "\n",
    "标量运算符的特点是对张量实施逐元素运算。\n",
    "\n",
    "有些标量运算符对常用的数学运算符进行了重载。并且支持类似numpy的广播特性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  8.],\n",
       "        [ 4., 12.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1.0, 2], [-3, 4.0]])\n",
    "b = torch.tensor([[5.0, 6], [7.0, 8.0]])\n",
    "a + b  # 运算符重载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -4.,  -4.],\n",
       "        [-10.,  -4.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.,  12.],\n",
       "        [-21.,  32.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2000,  0.3333],\n",
       "        [-0.4286,  0.5000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  4.],\n",
       "        [ 9., 16.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.4142],\n",
       "        [   nan, 2.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求模\n",
    "\n",
    "a % 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.],\n",
       "        [-1.,  1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 向下取整出（地板除）\n",
    "a // 3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True],\n",
       "        [False,  True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a >= 2 # torch.ge(a, 2)  #ge: greater_equal缩写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True],\n",
       "        [False, False]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a >= 2) & (a <= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a >= 2) | (a <= 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False],\n",
       "        [False, False]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a == 5 # torch.eq(a, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.4142],\n",
       "        [   nan, 2.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12., 21.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 8.0])\n",
    "b = torch.tensor([5.0, 6.0])\n",
    "c = torch.tensor([6.0, 7.0])\n",
    "\n",
    "d = a + b + c\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 8.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3., -3.])\n",
      "tensor([ 2., -3.])\n",
      "tensor([ 3., -2.])\n",
      "tensor([ 2., -2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.6, -2.7])\n",
    "\n",
    "print(torch.round(x)) # 保留整数部分，四舍五入\n",
    "print(torch.floor(x)) # 保留整数部分，向下归整\n",
    "print(torch.ceil(x))  # 保留整数部分，向上归整\n",
    "print(torch.trunc(x)) # 保留整数部分，向0归整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6000, -0.7000])\n",
      "tensor([0.6000, 1.3000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.6, -2.7])\n",
    "print(torch.fmod(x, 2)) # 作除法取余数 \n",
    "print(torch.remainder(x, 2)) #作除法取剩余的部分，结果恒正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9000, -0.8000,  1.0000, -1.0000,  0.7000])\n",
      "tensor([  0.9000,  -0.8000,   1.0000, -20.0000,   0.7000])\n"
     ]
    }
   ],
   "source": [
    "# 幅值裁剪\n",
    "\n",
    "x = torch.tensor([0.9, -0.8, 100.0, -20.0, 0.7])\n",
    "y = torch.clamp(x, min = -1, max = 1)\n",
    "z = torch.clamp(x, max = 1)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、向量运算\n",
    "向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "tensor(45.)\n",
      "tensor(5.)\n",
      "tensor(9.)\n",
      "tensor(1.)\n",
      "tensor(362880.)\n",
      "tensor(2.7386)\n",
      "tensor(7.5000)\n",
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "#统计值\n",
    "\n",
    "a = torch.arange(1, 10).float()\n",
    "print(a)\n",
    "print(torch.sum(a))\n",
    "print(torch.mean(a))\n",
    "print(torch.max(a))\n",
    "print(torch.min(a))\n",
    "print(torch.prod(a)) # 累乘\n",
    "print(torch.std(a))  # 标准差\n",
    "print(torch.var(a))  # 方差\n",
    "print(torch.median(a)) # 中位数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7386)\n"
     ]
    }
   ],
   "source": [
    "# std\n",
    "\n",
    "a = torch.arange(1, 10).float()\n",
    "b = torch.zeros_like(a, dtype = torch.float) # 必须先初始化b\n",
    "b = torch.fill_(b, torch.mean(a))\n",
    "c = (a - b) ** 2\n",
    "std = torch.sqrt((torch.sum(c) / (len(a) - 1))) # 样本标准差除以n-1 总体标准差除以n\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "torch.return_types.max(\n",
      "values=tensor([7., 8., 9.]),\n",
      "indices=tensor([2, 2, 2]))\n",
      "torch.return_types.max(\n",
      "values=tensor([3., 6., 9.]),\n",
      "indices=tensor([2, 2, 2]))\n"
     ]
    }
   ],
   "source": [
    "#指定维度计算统计值\n",
    "\n",
    "b = a.view(3, 3)\n",
    "print(b)\n",
    "print(torch.max(b, dim = 0))\n",
    "print(torch.max(b, dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([ 1,  3,  6, 10, 15, 21, 28, 36, 45])\n",
      "tensor([     1,      2,      6,     24,    120,    720,   5040,  40320, 362880])\n"
     ]
    }
   ],
   "source": [
    "# cum\n",
    "\n",
    "a = torch.arange(1, 10)\n",
    "print(a)\n",
    "print(torch.cumsum(a, 0))\n",
    "print(torch.cumprod(a, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 7., 8.],\n",
      "        [1., 3., 2.],\n",
      "        [5., 6., 4.]]) \n",
      "\n",
      "torch.return_types.topk(\n",
      "values=tensor([[9., 7., 8.],\n",
      "        [5., 6., 4.]]),\n",
      "indices=tensor([[0, 0, 0],\n",
      "        [2, 2, 2]])) \n",
      "\n",
      "torch.return_types.topk(\n",
      "values=tensor([[9., 8.],\n",
      "        [3., 2.],\n",
      "        [6., 5.]]),\n",
      "indices=tensor([[0, 2],\n",
      "        [1, 2],\n",
      "        [1, 0]])) \n",
      "\n",
      "torch.return_types.sort(\n",
      "values=tensor([[7., 8., 9.],\n",
      "        [1., 2., 3.],\n",
      "        [4., 5., 6.]]),\n",
      "indices=tensor([[1, 2, 0],\n",
      "        [0, 2, 1],\n",
      "        [2, 0, 1]])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# torch.sort和torch.topk可以对张量排序\n",
    "\n",
    "# torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)\n",
    "# Returns the k largest elements of the given input tensor along a given dimension.\n",
    "# If largest is False then the k smallest elements are returned.\n",
    "\n",
    "a = torch.tensor([[9, 7, 8], [1, 3, 2], [5, 6, 4]]).float()\n",
    "print(a, \"\\n\")\n",
    "print(torch.topk(a, 2, dim = 0), \"\\n\") # 对dim1操作\n",
    "print(torch.topk(a, 2, dim = 1), \"\\n\") # 对dim0操作\n",
    "print(torch.sort(a, dim = 1), \"\\n\") # 只有dim1可变，dim0不变 -> 对dim0排序\n",
    "\n",
    "#利用torch.topk可以在Pytorch中实现KNN算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、矩阵运算\n",
    "矩阵必须是二维的。类似torch.tensor([1,2,3])这样的不是矩阵。\n",
    "\n",
    "矩阵运算包括：矩阵乘法，矩阵转置，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征值，矩阵分解等运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 4],\n",
      "        [6, 8]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[2, 0], [0, 2]])\n",
    "print(a @ b)  # 等价于torch.matmul(a, b) 或 torch.mm(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 3.],\n",
      "        [2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵转置\n",
    "a = torch.tensor([[1.0, 2], [3, 4]])\n",
    "print(a.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[-2.0000,  1.0000],\n",
      "        [ 1.5000, -0.5000]])\n",
      "tensor([[ 1.0000e+00,  0.0000e+00],\n",
      "        [-4.7684e-07,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵逆，必须为浮点类型\n",
    "\n",
    "a = torch.tensor([[1.0, 2], [3, 4]])\n",
    "inverse_a = torch.inverse(a)\n",
    "print(a)\n",
    "print(inverse_a)\n",
    "print(a @ inverse_a) # 结果是对的 = E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "# 矩阵求迹(trace, 主对角线各元素之和) \n",
    "\n",
    "a = torch.tensor([[1.0, 2], [3, 4]])\n",
    "print(torch.trace(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.4772)\n",
      "tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "# 矩阵求范数\n",
    "\n",
    "a = torch.tensor([[1.0, 2], [3, 4]])\n",
    "print(torch.norm(a)) # 默认求2范数\n",
    "print(torch.norm(a, p = 1)) # 求1范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.0000)\n"
     ]
    }
   ],
   "source": [
    "# 矩阵行列式\n",
    "a = torch.tensor([[1.0, 2], [3, 4]])\n",
    "print(torch.det(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.eig(\n",
      "eigenvalues=tensor([[ 2.5000,  2.7839],\n",
      "        [ 2.5000, -2.7839]]),\n",
      "eigenvectors=tensor([[ 0.2535, -0.4706],\n",
      "        [ 0.8452,  0.0000]]))\n"
     ]
    }
   ],
   "source": [
    "# 矩阵特征值和特征向量\n",
    "a = torch.tensor([[1.0, 2], [-5, 4]], dtype = torch.float)\n",
    "print(torch.eig(a, eigenvectors=True))\n",
    "\n",
    "#两个特征值分别是 -2.5+2.7839j, 2.5-2.7839j \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3162, -0.9487],\n",
      "        [-0.9487,  0.3162]]) \n",
      "\n",
      "tensor([[-3.1623, -4.4272],\n",
      "        [ 0.0000, -0.6325]]) \n",
      "\n",
      "tensor([[1.0000, 2.0000],\n",
      "        [3.0000, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵QR分解, 将一个方阵分解为一个正交矩阵q和上三角矩阵r\n",
    "# QR分解实际上是对矩阵a实施Schmidt正交化得到q\n",
    "\n",
    "a  = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "q, r = torch.qr(a)\n",
    "print(q, \"\\n\")\n",
    "print(r, \"\\n\")\n",
    "print(q @ r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2298,  0.8835],\n",
      "        [-0.5247,  0.2408],\n",
      "        [-0.8196, -0.4019]]) \n",
      "\n",
      "tensor([9.5255, 0.5143]) \n",
      "\n",
      "tensor([[-0.6196, -0.7849],\n",
      "        [-0.7849,  0.6196]]) \n",
      "\n",
      "tensor([[1.0000, 2.0000],\n",
      "        [3.0000, 4.0000],\n",
      "        [5.0000, 6.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵svd分解\n",
    "# svd分解可以将任意一个矩阵分解为一个正交矩阵u,一个对角阵s和一个正交矩阵v.t()的乘积\n",
    "# svd常用于矩阵压缩和降维\n",
    "\n",
    "a=torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "\n",
    "u, s, v = torch.svd(a)\n",
    "\n",
    "print(u, \"\\n\")\n",
    "print(s, \"\\n\")\n",
    "print(v, \"\\n\")\n",
    "\n",
    "print(u @ torch.diag(s) @ v.t())\n",
    "\n",
    "#利用svd分解可以在Pytorch中实现主成分分析降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、广播机制\n",
    "Pytorch的广播规则和numpy是一样的:\n",
    "\n",
    "1、如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。 <p>\n",
    "2、如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1，那么我们就说这两个张量在该维度上是相容的。<p>\n",
    "3、如果两个张量在所有维度上都是相容的，它们就能使用广播。<p>\n",
    "4、广播之后，每个维度的长度将取两个张量在该维度长度的较大值。<p>\n",
    "5、在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。<p>\n",
    "torch.broadcast_tensors可以将多个张量根据广播规则转换成相同的维度。<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [2, 3, 4],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([[0, 0, 0], [1, 1, 1], [2, 2, 2]])\n",
    "print(b + a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3]]) \n",
      "\n",
      "tensor([[0, 0, 0],\n",
      "        [1, 1, 1],\n",
      "        [2, 2, 2]]) \n",
      "\n",
      "tensor([[1, 2, 3],\n",
      "        [2, 3, 4],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "a_broad, b_broad = torch.broadcast_tensors(a, b)\n",
    "print(a_broad, \"\\n\")\n",
    "print(b_broad, \"\\n\")\n",
    "print(a_broad + b_broad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
